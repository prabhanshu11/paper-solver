# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_preprocess.ipynb.

# %% auto 0
__all__ = ['TEST_SIZE', 'PROJECT_HOME', 'INPUT_PATH', 'OUTPUT_PATH', 'files', 'zip_dir_name', 'images', 'words', 'bboxes',
           'ner_tags', 'image_path', 'labels', 'id2label', 'label2id', 'dataset_dict', 'features', 'read_text_file',
           'prepare_examples', 'get_zip_dir_name', 'filter_out_unannotated']

# %% ../nbs/01_preprocess.ipynb 5
import pandas as pd
import numpy as np
import os, argparse
from pathlib import Path
from datasets.features import ClassLabel
from transformers import AutoProcessor
from sklearn.model_selection import train_test_split
from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D, Dataset
from datasets import Image as Img
from PIL import Image
import warnings
from typing import Union
#warnings.filterwarnings('ignore')

# %% ../nbs/01_preprocess.ipynb 7
import pandas as pd
import numpy as np
import os, argparse
from pathlib import Path
from datasets.features import ClassLabel
from transformers import AutoProcessor
from sklearn.model_selection import train_test_split
from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D, Dataset
from datasets import Image as Img
from PIL import Image
import warnings
from typing import Union
#warnings.filterwarnings('ignore')

# %% ../nbs/01_preprocess.ipynb 8
def read_text_file(file_path):
    with open(file_path, 'r') as f:
        return (f.readlines())

def prepare_examples(examples):
    images = examples[image_column_name]
    words = examples[text_column_name]
    boxes = examples[boxes_column_name]
    word_labels = examples[label_column_name]
    encoding = processor(images, words, 
      boxes=boxes, word_labels=word_labels,
      truncation=True, padding="max_length"
                      )
    return encoding

def get_zip_dir_name(data_directory: Union[str, Path]) -> Union[str, bool]:
    data_path = Path(data_directory)
    dir_list = [f.name for f in data_path.iterdir() if f.is_dir()]
    zip_dir_name = dir_list[0]
    if all([f.startswith(zip_dir_name) for f in dir_list]):
        return zip_dir_name
    return False

def filter_out_unannotated(example):
    tags = example['ner_tags']
    return not all([tag == label2id['O'] for tag in tags])

# %% ../nbs/01_preprocess.ipynb 10
TEST_SIZE = 0.33
PROJECT_HOME = Path('..')
print(f"""****************************************
project home exist ? {PROJECT_HOME.exists()}
contents = {[i.__str__()[3:] for i in list(PROJECT_HOME.iterdir())]}""")

INPUT_PATH = PROJECT_HOME/Path('data/doc-scanner/')
print(f"""INPUT_PATH exist ? {INPUT_PATH.exists()}
contents = {[i.__str__()[3:] for i in list(INPUT_PATH.iterdir())]}""")

OUTPUT_PATH = PROJECT_HOME/Path('data/preprocessed/')
OUTPUT_PATH.mkdir(parents=True, exist_ok=True)

# %% ../nbs/01_preprocess.ipynb 12
files = {}
zip_dir_name = get_zip_dir_name(INPUT_PATH)

print('zip_dir_name', zip_dir_name)
if zip_dir_name:
    data_path = INPUT_PATH / zip_dir_name
    files['train_box']   = read_text_file(data_path / f'{zip_dir_name}_box.txt')
    files['train_image'] = read_text_file(data_path / f'{zip_dir_name}_image.txt')
    files['train']       = read_text_file(data_path / f'{zip_dir_name}.txt')
else:
    for f in Path('.').iterdir():
        if f.suffix == '.txt' and 'box' in f.name:
            files['train_box'] = read_text_file(f)
        elif f.suffix == '.txt' and 'image' in f.name:
            files['train_image'] = read_text_file(f)
        elif f.suffix == '.txt' and 'labels' not in f.name:
            files['train'] = read_text_file(f)

            
assert(len(files['train']) == len(files['train_box']))
assert(len(files['train_box']) == len(files['train_image']))
assert(len(files['train_image']) == len(files['train']))

# %% ../nbs/01_preprocess.ipynb 13
print('Length of box, image and txt', list(map(len, map(files.get, files.keys()))))

# %% ../nbs/01_preprocess.ipynb 14
images = {}
for i, row in enumerate(files['train_image']):
    if row != '\n':
        image_name = row.split('\t')[-1]
        images.setdefault(image_name.replace('\n', ''), []).append(i)

# %% ../nbs/01_preprocess.ipynb 18
words, bboxes, ner_tags, image_path = [], [], [], []
for image, rows in images.items():
    words.append([row.split('\t')[0].replace('\n', '')
                 for row in files['train'][rows[0]:rows[-1]+1]])
    ner_tags.append([row.split('\t')[1].replace('\n', '')
                    for row in files['train'][rows[0]:rows[-1]+1]])
    bboxes.append([box.split('\t')[1].replace('\n', '')
                  for box in files['train_box'][rows[0]:rows[-1]+1]])
    image_path.append(str(data_path/image))

# %% ../nbs/01_preprocess.ipynb 23
labels = list(set(tag for ner_tag in ner_tags for tag in ner_tag))
id2label = {v: k for v, k in enumerate(labels)}
label2id = {k: v for v, k in enumerate(labels)}

dataset_dict = {
    'id': range(len(words)),
    'tokens': words,
    'bboxes': [[list(map(int, bbox.split())) for bbox in doc] for doc in bboxes],
    'ner_tags': [[label2id[tag] for tag in ner_tag] for ner_tag in ner_tags],
    'image': [Image.open(path).convert("RGB") for path in image_path]
}

#raw features
features = Features({
    'id': Value(dtype='string', id=None),
    'tokens': Sequence(feature=Value(dtype='string', id=None), 
                       length=-1, id=None),
    'bboxes': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), 
                                        length=-1, id=None), 
                       length=-1, id=None),
    'ner_tags': Sequence(feature=ClassLabel(num_classes=len(labels), 
                                            names=labels, 
                                            names_file=None, id=None),
                         length=-1, id=None),
    'image': Img(decode=True, id=None)
})

# %% ../nbs/01_preprocess.ipynb 26
if __name__ == "__main__":
    full_data_set = Dataset.from_dict(dataset_dict, features=features)
    dataset = full_data_set.train_test_split(test_size=TEST_SIZE)
    dataset["train"] = dataset["train"].filter(filter_out_unannotated)
    processor = AutoProcessor.from_pretrained(
        "microsoft/layoutlmv3-base", apply_ocr=False)
    features = dataset["train"].features
    column_names = dataset["train"].column_names
    image_column_name = "image"
    text_column_name = "tokens"
    boxes_column_name = "bboxes"
    label_column_name = "ner_tags"

    features = Features({
        'pixel_values': Array3D(dtype="float32", shape=(3, 224, 224)),
        'input_ids': Sequence(feature=Value(dtype='int64')),
        'attention_mask': Sequence(Value(dtype='int64')),
        'bbox': Array2D(dtype="int64", shape=(512, 4)),
        'labels': Sequence(ClassLabel(names=labels)),
    })
    train_dataset = dataset["train"].map(
        prepare_examples,
        batched=True,
        remove_columns=column_names,
        features=features,
    )
    eval_dataset = dataset["test"].map(
        prepare_examples,
        batched=True,
        remove_columns=column_names,
        features=features,
    )
    train_dataset.set_format("torch")
    train_dataset.save_to_disk(OUTPUT_PATH/'train_split')
    eval_dataset.save_to_disk(OUTPUT_PATH/'eval_split')
    dataset.save_to_disk(OUTPUT_PATH/'raw_data')
