{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, argparse\n",
    "from pathlib import Path\n",
    "from datasets.features import ClassLabel\n",
    "from transformers import AutoProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D, Dataset\n",
    "from datasets import Image as Img\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from typing import Union\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, argparse\n",
    "from pathlib import Path\n",
    "from datasets.features import ClassLabel\n",
    "from transformers import AutoProcessor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D, Dataset\n",
    "from datasets import Image as Img\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from typing import Union\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return (f.readlines())\n",
    "\n",
    "def prepare_examples(examples):\n",
    "    images = examples[image_column_name]\n",
    "    words = examples[text_column_name]\n",
    "    boxes = examples[boxes_column_name]\n",
    "    word_labels = examples[label_column_name]\n",
    "    encoding = processor(images, words, \n",
    "      boxes=boxes, word_labels=word_labels,\n",
    "      truncation=True, padding=\"max_length\"\n",
    "                      )\n",
    "    return encoding\n",
    "\n",
    "def get_zip_dir_name(data_directory: Union[str, Path]) -> Union[str, bool]:\n",
    "    data_path = Path(data_directory)\n",
    "    dir_list = [f.name for f in data_path.iterdir() if f.is_dir()]\n",
    "    zip_dir_name = dir_list[0]\n",
    "    if all([f.startswith(zip_dir_name) for f in dir_list]):\n",
    "        return zip_dir_name\n",
    "    return False\n",
    "\n",
    "def filter_out_unannotated(example):\n",
    "    tags = example['ner_tags']\n",
    "    return not all([tag == label2id['O'] for tag in tags])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "project home exist ? True\n",
      "contents = ['.git', '.gitignore', 'README.md', 'nbs', '.github', 'LICENSE', 'MANIFEST.in', 'setup.py', 'paper_solver', 'nbdev-template-1.1.1', '.ipynb_checkpoints', '.gitconfig', '.gitattributes', 'paper_solver.egg-info', '_proc', 'data', 'preprocessed', 'settings.ini']\n",
      "INPUT_PATH exist ? True\n",
      "contents = ['data/doc-scanner/5fe15b06-ee59-4461-9f88-505f3e4b2696']\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "TEST_SIZE = 0.33\n",
    "PROJECT_HOME = Path('..')\n",
    "print(f\"\"\"****************************************\n",
    "project home exist ? {PROJECT_HOME.exists()}\n",
    "contents = {[i.__str__()[3:] for i in list(PROJECT_HOME.iterdir())]}\"\"\")\n",
    "\n",
    "INPUT_PATH = PROJECT_HOME/Path('data/doc-scanner/')\n",
    "print(f\"\"\"INPUT_PATH exist ? {INPUT_PATH.exists()}\n",
    "contents = {[i.__str__()[3:] for i in list(INPUT_PATH.iterdir())]}\"\"\")\n",
    "\n",
    "OUTPUT_PATH = PROJECT_HOME/Path('data/preprocessed/')\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_PATH.exists(), OUTPUT_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zip_dir_name 5fe15b06-ee59-4461-9f88-505f3e4b2696\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "files = {}\n",
    "zip_dir_name = get_zip_dir_name(INPUT_PATH)\n",
    "\n",
    "print('zip_dir_name', zip_dir_name)\n",
    "if zip_dir_name:\n",
    "    data_path = INPUT_PATH / zip_dir_name\n",
    "    files['train_box']   = read_text_file(data_path / f'{zip_dir_name}_box.txt')\n",
    "    files['train_image'] = read_text_file(data_path / f'{zip_dir_name}_image.txt')\n",
    "    files['train']       = read_text_file(data_path / f'{zip_dir_name}.txt')\n",
    "else:\n",
    "    for f in Path('.').iterdir():\n",
    "        if f.suffix == '.txt' and 'box' in f.name:\n",
    "            files['train_box'] = read_text_file(f)\n",
    "        elif f.suffix == '.txt' and 'image' in f.name:\n",
    "            files['train_image'] = read_text_file(f)\n",
    "        elif f.suffix == '.txt' and 'labels' not in f.name:\n",
    "            files['train'] = read_text_file(f)\n",
    "\n",
    "            \n",
    "assert(len(files['train']) == len(files['train_box']))\n",
    "assert(len(files['train_box']) == len(files['train_image']))\n",
    "assert(len(files['train_image']) == len(files['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of box, image and txt [1415, 1415, 1415]\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "print('Length of box, image and txt', list(map(len, map(files.get, files.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "images = {}\n",
    "for i, row in enumerate(files['train_image']):\n",
    "    if row != '\\n':\n",
    "        image_name = row.split('\\t')[-1]\n",
    "        images.setdefault(image_name.replace('\\n', ''), []).append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_box</th>\n",
       "      <th>train_image</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FRESHERS\\t781 132 839 140\\n</td>\n",
       "      <td>FRESHERS\\t781 132 839 140\\t1700 2200\\tpage_12_...</td>\n",
       "      <td>FRESHERS\\tO\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NOW\\t843 132 876 140\\n</td>\n",
       "      <td>NOW\\t843 132 876 140\\t1700 2200\\tpage_12_image...</td>\n",
       "      <td>NOW\\tO\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b)\\t116 227 135 243\\n</td>\n",
       "      <td>b)\\t116 227 135 243\\t1700 2200\\tpage_12_image_...</td>\n",
       "      <td>b)\\tB-SUB-SUB-Q\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8\\t143 227 153 240\\n</td>\n",
       "      <td>8\\t143 227 153 240\\t1700 2200\\tpage_12_image_0...</td>\n",
       "      <td>8\\tI-SUB-SUB-Q\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c)\\t117 268 134 285\\n</td>\n",
       "      <td>c)\\t117 268 134 285\\t1700 2200\\tpage_12_image_...</td>\n",
       "      <td>c)\\tI-SUB-SUB-Q\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     train_box  \\\n",
       "0  FRESHERS\\t781 132 839 140\\n   \n",
       "1       NOW\\t843 132 876 140\\n   \n",
       "2        b)\\t116 227 135 243\\n   \n",
       "3         8\\t143 227 153 240\\n   \n",
       "4        c)\\t117 268 134 285\\n   \n",
       "\n",
       "                                         train_image              train  \n",
       "0  FRESHERS\\t781 132 839 140\\t1700 2200\\tpage_12_...      FRESHERS\\tO\\n  \n",
       "1  NOW\\t843 132 876 140\\t1700 2200\\tpage_12_image...           NOW\\tO\\n  \n",
       "2  b)\\t116 227 135 243\\t1700 2200\\tpage_12_image_...  b)\\tB-SUB-SUB-Q\\n  \n",
       "3  8\\t143 227 153 240\\t1700 2200\\tpage_12_image_0...   8\\tI-SUB-SUB-Q\\n  \n",
       "4  c)\\t117 268 134 285\\t1700 2200\\tpage_12_image_...  c)\\tI-SUB-SUB-Q\\n  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(files).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "page_12_image_0.jpg    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "page_11_image_0.jpg    [119, 120, 121, 122, 123, 124, 125, 126, 127, ...\n",
       "page_10_image_0.jpg    [288, 289, 290, 291, 292, 293, 294, 295, 296, ...\n",
       "page_9_image_0.jpg     [366, 367, 368, 369, 370, 371, 372, 373, 374, ...\n",
       "page_8_image_0.jpg     [482, 483, 484, 485, 486, 487, 488, 489, 490, ...\n",
       "page_7_image_0.jpg     [546, 547, 548, 549, 550, 551, 552, 553, 554, ...\n",
       "page_6_image_0.jpg     [623, 624, 625, 626, 627, 628, 629, 630, 631, ...\n",
       "page_5_image_0.jpg     [721, 722, 723, 724, 725, 726, 727, 728, 729, ...\n",
       "page_4_image_0.jpg     [797, 798, 799, 800, 801, 802, 803, 804, 805, ...\n",
       "page_3_image_0.jpg     [956, 957, 958, 959, 960, 961, 962, 963, 964, ...\n",
       "page_2_image_0.jpg     [1062, 1063, 1064, 1065, 1066, 1067, 1068, 106...\n",
       "page_1_image_0.jpg     [1204, 1205, 1206, 1207, 1208, 1209, 1210, 121...\n",
       "dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "words, bboxes, ner_tags, image_path = [], [], [], []\n",
    "for image, rows in images.items():\n",
    "    words.append([row.split('\\t')[0].replace('\\n', '')\n",
    "                 for row in files['train'][rows[0]:rows[-1]+1]])\n",
    "    ner_tags.append([row.split('\\t')[1].replace('\\n', '')\n",
    "                    for row in files['train'][rows[0]:rows[-1]+1]])\n",
    "    bboxes.append([box.split('\\t')[1].replace('\\n', '')\n",
    "                  for box in files['train_box'][rows[0]:rows[-1]+1]])\n",
    "    image_path.append(str(data_path/image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "1     ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "2     ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "3     ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "4     ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "5     ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "6     ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "7     ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "8     ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "9     ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "10    ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "11    ../data/doc-scanner/5fe15b06-ee59-4461-9f88-50...\n",
       "dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [O, O, B-SUB-SUB-Q, I-SUB-SUB-Q, I-SUB-SUB-Q, ...\n",
       "1     [O, O, B-Q, I-Q, I-Q, I-Q, I-Q, I-Q, I-Q, I-Q,...\n",
       "2     [O, O, B-SUB-SUB-Q, I-SUB-SUB-Q, I-SUB-SUB-Q, ...\n",
       "3     [O, O, B-SUB-Q, I-SUB-Q, I-SUB-Q, I-SUB-Q, I-S...\n",
       "4     [O, O, B-CHART, I-CHART, I-CHART, I-CHART, I-C...\n",
       "5     [O, O, B-SUB-SUB-Q, I-SUB-SUB-Q, I-SUB-SUB-Q, ...\n",
       "6     [O, O, B-SUB-SUB-Q, I-SUB-SUB-Q, I-SUB-SUB-Q, ...\n",
       "7     [O, O, B-SUB-SUB-Q, I-SUB-SUB-Q, I-SUB-SUB-Q, ...\n",
       "8     [O, O, B-Q, I-Q, I-Q, I-Q, I-Q, I-Q, I-Q, I-Q,...\n",
       "9     [O, O, B-SUB-Q, I-SUB-Q, I-SUB-Q, I-SUB-Q, I-S...\n",
       "10    [O, O, B-SUB-SUB-Q, I-SUB-SUB-Q, I-SUB-SUB-Q, ...\n",
       "11    [O, O, B-SUBJECT NAME, E-SUBJECT NAME, B-Q, E-...\n",
       "dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                O\n",
       "1                O\n",
       "2      B-SUB-SUB-Q\n",
       "3      I-SUB-SUB-Q\n",
       "4      I-SUB-SUB-Q\n",
       "          ...     \n",
       "113            I-Q\n",
       "114            I-Q\n",
       "115            I-Q\n",
       "116            I-Q\n",
       "117            E-Q\n",
       "Length: 118, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(ner_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating features from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "labels = list(set(tag for ner_tag in ner_tags for tag in ner_tag))\n",
    "id2label = {v: k for v, k in enumerate(labels)}\n",
    "label2id = {k: v for v, k in enumerate(labels)}\n",
    "\n",
    "dataset_dict = {\n",
    "    'id': range(len(words)),\n",
    "    'tokens': words,\n",
    "    'bboxes': [[list(map(int, bbox.split())) for bbox in doc] for doc in bboxes],\n",
    "    'ner_tags': [[label2id[tag] for tag in ner_tag] for ner_tag in ner_tags],\n",
    "    'image': [Image.open(path).convert(\"RGB\") for path in image_path]\n",
    "}\n",
    "\n",
    "#raw features\n",
    "features = Features({\n",
    "    'id': Value(dtype='string', id=None),\n",
    "    'tokens': Sequence(feature=Value(dtype='string', id=None), \n",
    "                       length=-1, id=None),\n",
    "    'bboxes': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), \n",
    "                                        length=-1, id=None), \n",
    "                       length=-1, id=None),\n",
    "    'ner_tags': Sequence(feature=ClassLabel(num_classes=len(labels), \n",
    "                                            names=labels, \n",
    "                                            names_file=None, id=None),\n",
    "                         length=-1, id=None),\n",
    "    'image': Img(decode=True, id=None)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../data/preprocessed')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| export\n",
    "if __name__ == \"__main__\":\n",
    "    full_data_set = Dataset.from_dict(dataset_dict, features=features)\n",
    "    dataset = full_data_set.train_test_split(test_size=TEST_SIZE)\n",
    "    dataset[\"train\"] = dataset[\"train\"].filter(filter_out_unannotated)\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        \"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "    features = dataset[\"train\"].features\n",
    "    column_names = dataset[\"train\"].column_names\n",
    "    image_column_name = \"image\"\n",
    "    text_column_name = \"tokens\"\n",
    "    boxes_column_name = \"bboxes\"\n",
    "    label_column_name = \"ner_tags\"\n",
    "\n",
    "    features = Features({\n",
    "        'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "        'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "        'attention_mask': Sequence(Value(dtype='int64')),\n",
    "        'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "        'labels': Sequence(ClassLabel(names=labels)),\n",
    "    })\n",
    "    train_dataset = dataset[\"train\"].map(\n",
    "        prepare_examples,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "        features=features,\n",
    "    )\n",
    "    eval_dataset = dataset[\"test\"].map(\n",
    "        prepare_examples,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "        features=features,\n",
    "    )\n",
    "    train_dataset.set_format(\"torch\")\n",
    "    train_dataset.save_to_disk(OUTPUT_PATH/'train_split')\n",
    "    eval_dataset.save_to_disk(OUTPUT_PATH/'eval_split')\n",
    "    dataset.save_to_disk(OUTPUT_PATH/'raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
